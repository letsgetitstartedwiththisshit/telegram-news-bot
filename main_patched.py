import os
import feedparser
from telegram import Update, InlineKeyboardButton, InlineKeyboardMarkup
from telegram.ext import (
    Application,
    ContextTypes,
    CallbackQueryHandler,
    MessageHandler,
    filters,
)
import asyncio
import uuid
import requests
from bs4 import BeautifulSoup

# Additional imports for filtering and translation
import re
from urllib.parse import urlparse
from deep_translator import LibreTranslator, GoogleTranslator

TOKEN = os.getenv("TELEGRAM_BOT_TOKEN")
VALIDATION_CHANNEL_ID = int(os.getenv("VALIDATION_CHANNEL_ID", "0"))
MAIN_CHANNEL_ID = int(os.getenv("MAIN_CHANNEL_ID", "0"))
SOURCES = [s.strip() for s in os.getenv("SOURCES", "").split(",") if s.strip()]

# Track canonical links we've already seen to avoid duplicates.
posted_links = set()
# pending_posts maps callback IDs to a dict with keys:
# 'text': the message text sent to the validation channel (translated and formatted),
# 'image_url': an optional image URL to post with the message,
# 'validator_message_id': the Telegram message ID of the post in the validation channel,
# 'canonical_link': the canonicalised URL used for deduplication,
# 'source_link': the original article link.
pending_posts = {}

# In the simplified version (no edit propagation), we no longer track posted posts
# separately or map validator message IDs back to callback IDs.  These features
# were used to support live edit propagation between the validation and main
# channels.  Since we're reverting to a pre-edit workflow, they are no longer
# needed.

# Query parameters to drop when canonicalizing URLs
DROP_PARAMS = {
    "utm_source", "utm_medium", "utm_campaign", "utm_term", "utm_content",
    "fbclid", "gclid", "mc_cid", "mc_eid"
}

def canonicalize_url(u: str) -> str:
    """Return a canonical form of the URL for deduplication.

    Removes tracking parameters, strips fragments, normalizes host and path,
    and ensures a consistent scheme.
    """
    try:
        from urllib.parse import urlparse, urlunparse, parse_qsl, urlencode
        p = urlparse(u)
        # filter query parameters
        q = [(k, v) for k, v in parse_qsl(p.query, keep_blank_values=True) if k.lower() not in DROP_PARAMS]
        # normalize path (remove trailing slash, ensure at least '/')
        path = p.path.rstrip("/") or "/"
        return urlunparse((
            p.scheme.lower(),
            p.netloc.lower(),
            path,
            "",  # params unused
            urlencode(q, doseq=True),
            ""   # fragment removed
        ))
    except Exception:
        return u

# -----------------------------------------------------------------------------
# International source filtering and translation helpers
#
# The bot monitors both Ukrainian and international RSS feeds. For international
# sources we only want to forward items that are related to Ukraine. To detect
# relevant items we look for Ukraine-related keywords in the title and summary.
# If an item is relevant, we translate the title and a short portion of the
# summary into Ukrainian using a free translation service (LibreTranslate or
# GoogleTranslator via deep_translator). The original Ukrainian feeds are not
# translated.

# Snippets of hostnames that identify international sources. Any feed whose
# hostname contains one of these snippets will be treated as international.
INTL_HOST_SNIPPETS = (
    "bbc.", "reuters.", "theguardian.", "apnews.", "cnn.", "aljazeera.",
    "nytimes.", "dw.com", "npr.org", "euronews."
)

# Regular expression to match Ukraine-related keywords in multiple languages.
# Matches both Ukrainian and English spellings of key terms and cities.
UA_REGEX = re.compile(
    r"(Ð£ÐºÑ€Ð°Ñ—Ð½Ð°|ÑƒÐºÑ€Ð°Ñ—Ð½|ÐšÐ¸Ñ—Ð²|ÐšÐ¸Ñ”Ð²|Ð¥Ð°Ñ€ÐºÑ–Ð²|Ð›ÑŒÐ²Ñ–Ð²|ÐžÐ´ÐµÑÐ°|Ð”Ð¾Ð½Ð±Ð°Ñ|Ð”Ð¾Ð½ÐµÑ†ÑŒÐº|ÐšÑ€Ð¸Ð¼|"
    r"Ð¥ÐµÑ€ÑÐ¾Ð½|ÐœÐ°Ñ€Ñ–ÑƒÐ¿Ð¾Ð»ÑŒ|Ð—Ð°Ð¿Ð¾Ñ€Ñ–Ð¶Ð¶|Ð”Ð½Ñ–Ð¿Ñ€Ð¾|Ð—ÐµÐ»ÐµÐ½ÑÑŒÐº|"
    r"Ð£ÐºÑ€Ð°Ð¸Ð½Ð°|ÐšÐ¸ÐµÐ²|Ð¥Ð°Ñ€ÑŒÐºÐ¾Ð²|Ð›ÑŒÐ²Ð¾Ð²|ÐžÐ´ÐµÑÑÐ°|"
    r"Ukraine|Ukrainian|Kyiv|Kiev|Kharkiv|Lviv|Odesa|Donbas|Donetsk|"
    r"Crimea|Kherson|Mariupol|Zaporizh|Dnipro|Zelensky)",
    re.IGNORECASE
)

def is_international(url: str) -> bool:
    """Return True if the given link belongs to an international source.

    We determine this by checking the link's hostname for known snippets.
    """
    try:
        host = urlparse(url).netloc.lower()
        return any(snippet in host for snippet in INTL_HOST_SNIPPETS)
    except Exception:
        return False

def is_ukraine_related(title: str, summary: str) -> bool:
    """Return True if the title or summary contains Ukraine-related keywords."""
    haystack = f"{title or ''} {summary or ''}"
    return bool(UA_REGEX.search(haystack))

def translate_to_uk(text: str) -> str:
    """Translate arbitrary text into Ukrainian using a free service.

    This tries LibreTranslator first and falls back to GoogleTranslator.
    If both fail, the original text is returned unchanged.
    """
    if not text:
        return text
    for translator in (
        lambda t: LibreTranslator(source="auto", target="uk").translate(t),
        lambda t: GoogleTranslator(source="auto", target="uk").translate(t),
    ):
        try:
            return translator(text)
        except Exception:
            continue
    return text

def build_validation_message(link: str, title: str, summary: str) -> str:
    """Construct the message sent to the validation channel.

    For international sources, the title and a brief summary (up to 300
    characters) are translated into Ukrainian. For Ukrainian sources,
    the original title and summary are used. A link to the source is always
    appended. The return value is formatted in Markdown.
    """
    if is_international(link):
        # Only forward if it's Ukraine-related
        if not is_ukraine_related(title, summary):
            return ""
        brief = (summary or "")[:300]
        translated_title = translate_to_uk(title or "")
        translated_brief = translate_to_uk(brief) if brief else ""
        lines = [f"ðŸ‡ºðŸ‡¦ *{translated_title}*"]
        if translated_brief:
            lines.append(translated_brief)
        lines.append(f"ðŸ”— Ð”Ð¶ÐµÑ€ÐµÐ»Ð¾: {link}")
        return "\n".join(lines)
    else:
        # Ukrainian sources: keep original text
        cap_lines = []
        if title:
            cap_lines.append(f"*{title}*")
        if summary:
            cap_lines.append(summary)
        cap_lines.append(f"\n[Ð§Ð¸Ñ‚Ð°Ñ‚Ð¸ Ð´Ð¶ÐµÑ€ÐµÐ»Ð¾]({link})")
        return "\n".join(cap_lines)

def extract_image(entry):
    """
    Try to extract an image URL from an RSS entry.
    Priority:
      1. media:content or media:thumbnail
      2. enclosure link with image MIME type
      3. og:image from the linked article's HTML
    Returns None if no image is found.
    """
    # media:content or media:thumbnail
    media = entry.get("media_content") or []
    if media:
        url = media[0].get("url")
        if url:
            return url
    thumbs = entry.get("media_thumbnail") or []
    if thumbs:
        url = thumbs[0].get("url")
        if url:
            return url
    # enclosure links
    for link_info in entry.get("links", []):
        if link_info.get("rel") == "enclosure" and link_info.get("type", "").startswith("image/"):
            href = link_info.get("href")
            if href:
                return href
    # fallback: scrape og:image from article page
    try:
        html = requests.get(entry.link, timeout=6).text
        soup = BeautifulSoup(html, "html.parser")
        og = soup.find("meta", property="og:image")
        if og and og.get("content"):
            return og["content"]
    except Exception:
        pass
    return None

async def fetch_feeds(context: ContextTypes.DEFAULT_TYPE):
    bot = context.bot
    for url in SOURCES:
        feed = feedparser.parse(url)
        for entry in feed.entries[:5]:
            link = entry.link
            # Skip if already processed based on canonical URL
            canonical_link = canonicalize_url(link)
            if canonical_link in posted_links:
                continue
            posted_links.add(canonical_link)
            title = entry.title
            summary = entry.get("summary", "")
            # Build a validation message with translation and filtering
            message = build_validation_message(link, title, summary)
            # Skip if the international item isn't Ukraine-related (empty message)
            if not message:
                continue
            image_url = extract_image(entry)
            # Generate a short unique callback ID instead of using the full link
            callback_id = uuid.uuid4().hex
            keyboard = InlineKeyboardMarkup([
                [
                    InlineKeyboardButton("âœ… ÐŸÑƒÐ±Ð»Ñ–ÐºÑƒÐ²Ð°Ñ‚Ð¸", callback_data=f"approve:{callback_id}"),
                    InlineKeyboardButton("âŒ Ð’Ñ–Ð´Ñ…Ð¸Ð»Ð¸Ñ‚Ð¸", callback_data=f"reject:{callback_id}"),
                ]
            ])
            # Send the message to the validation channel and capture the message ID
            sent_msg = await bot.send_message(
                chat_id=VALIDATION_CHANNEL_ID,
                text=message,
                parse_mode="Markdown",
                reply_markup=keyboard,
                disable_web_page_preview=True,
            )
            # Save pending post metadata.  Store the original link so we can
            # always include it in the caption even if the summary is
            # truncated when posting to the main channel.
            # Store the post metadata.  We keep only the fields needed to
            # construct the final post: the original message text, the image
            # URL (if any), the canonical link for deduplication, and the
            # original source link.  We no longer track edited_text,
            # main_message_id or status, since edit propagation is disabled.
            pending_posts[callback_id] = {
                "text": message,
                "image_url": image_url,
                "validator_message_id": sent_msg.message_id,
                "canonical_link": canonical_link,
                "source_link": link,
            }

async def handle_callback(update: Update, context: ContextTypes.DEFAULT_TYPE):
    query = update.callback_query
    await query.answer()
    if not query.data:
        return
    if ":" not in query.data:
        return
    action, callback_id = query.data.split(":", 1)
    # Fetch the pending post data.  Since edit propagation is disabled, we
    # only look up the post in `pending_posts`.  If it's missing, the post
    # has already been processed.
    data = pending_posts.get(callback_id)
    if not data:
        await query.edit_message_text("Ð¦ÐµÐ¹ Ð·Ð°Ð¿Ð¸Ñ Ð²Ð¶Ðµ Ð¾Ð¿Ñ€Ð°Ñ†ÑŒÐ¾Ð²Ð°Ð½Ð¾.")
        return
    original_text = data["text"]
    image_url = data.get("image_url")
    # Use the stored source link; fall back to the canonical link if needed.
    source_link = data.get("source_link") or data.get("canonical_link")
    # Always append the source link on a new line when posting to the main channel.
    link_line = f"\n\nðŸ”— Ð”Ð¶ÐµÑ€ÐµÐ»Ð¾: {source_link}"
    if action == "approve":
        if image_url:
            # For photos, limit captions to 1024 characters including the link.
            max_len = 1024 - len(link_line)
            if len(original_text) > max_len:
                base_caption = original_text[: max_len - 3] + "..."
            else:
                base_caption = original_text
            caption = base_caption + link_line
            await context.bot.send_photo(
                chat_id=MAIN_CHANNEL_ID,
                photo=image_url,
                caption=caption,
                parse_mode="Markdown",
            )
            status = "âœ… ÐžÐ¿ÑƒÐ±Ð»Ñ–ÐºÐ¾Ð²Ð°Ð½Ð¾"
        else:
            # For text messages, limit to 4096 characters including the link.
            max_len = 4096 - len(link_line)
            if len(original_text) > max_len:
                base_text = original_text[: max_len - 3] + "..."
            else:
                base_text = original_text
            text_to_send = base_text + link_line
            await context.bot.send_message(
                chat_id=MAIN_CHANNEL_ID,
                text=text_to_send,
                parse_mode="Markdown",
                disable_web_page_preview=False,
            )
            status = "âœ… ÐžÐ¿ÑƒÐ±Ð»Ñ–ÐºÐ¾Ð²Ð°Ð½Ð¾"
    else:
        # No message sent to main channel on rejection
        status = "âŒ Ð’Ñ–Ð´Ñ…Ð¸Ð»ÐµÐ½Ð¾"
    # Edit the validation channel message to show the status and remove the buttons.
    await query.edit_message_text(
        original_text + f"\n\n{status}",
        parse_mode="Markdown",
        disable_web_page_preview=True,
    )
    # Remove the post from pending posts since it's now processed.
    pending_posts.pop(callback_id, None)


def main():
    if not TOKEN:
        raise RuntimeError("Missing TELEGRAM_BOT_TOKEN")
    # Build the application
    application = Application.builder().token(TOKEN).build()
    # Register the callback handler for approve/decline buttons
    application.add_handler(CallbackQueryHandler(handle_callback))
    # In the simplified workflow, we do not register handlers for edited
    # messages or channel posts.  Edits made in the validation channel will
    # not propagate to the main channel.  If an administrator needs to
    # correct a post, they should decline the original and resubmit a new
    # item.
    # Schedule periodic feed fetching every 10 minutes. The first run is after
    # 5 seconds to allow the bot to initialize properly.
    application.job_queue.run_repeating(fetch_feeds, interval=600, first=5)

    # Define a startup hook to delete any existing webhook and drop pending updates.
    async def remove_webhook(app: Application) -> None:
        try:
            # Delete webhook and drop any pending updates. This prevents conflicts
            # with concurrent getUpdates requests from other sessions.
            await app.bot.delete_webhook(drop_pending_updates=True)
        except Exception:
            # Ignore errors if webhook is not set or cannot be deleted.
            pass

    # Delete any existing webhook before starting polling to prevent
    # conflicts with previous getUpdates sessions. We cannot pass
    # `on_startup` to run_polling() because this PTB version does not
    # support that argument. Instead, run the webhook deletion here.
    import asyncio
    try:
        # Ensure any existing webhook is removed; drop pending updates.
        asyncio.run(application.bot.delete_webhook(drop_pending_updates=True))
    except Exception:
        # Ignore errors during webhook deletion; continue starting the bot.
        pass
    # Start polling. We disable signal handling because Render may manage
    # process signals itself. run_polling will initialize and start the
    # application, then cleanly shut down on exit.
    application.run_polling(close_loop=False)

if __name__ == "__main__":
    main()
